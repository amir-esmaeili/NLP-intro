{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "BBC NEWS | Health | Blondes 'to die out in 200 years'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NEWS\n",
      "  SPORT\n",
      "  WEATHER\n",
      "  WORLD SERVICE\n",
      "\n",
      "  A-Z INDEX \n",
      "\n",
      "  SEARCH \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "     You are in: Health  \r\n",
      "    \r\n",
      "    \r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "News Front Page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Africa\n",
      "\n",
      "\n",
      "Americas\n",
      "\n",
      "\n",
      "Asia-Pacific\n",
      "\n",
      "\n",
      "Europe\n",
      "\n",
      "\n",
      "Middle East\n",
      "\n",
      "\n",
      "South Asia\n",
      "\n",
      "\n",
      "UK\n",
      "\n",
      "\n",
      "Business\n",
      "\n",
      "\n",
      "Entertainment\n",
      "\n",
      "\n",
      "Science/Nature\n",
      "\n",
      "\n",
      "Technology\n",
      "\n",
      "\n",
      "Health\n",
      "\n",
      "\n",
      "Medical notes\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "\n",
      "Talking Point\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "\n",
      "Country Profiles\n",
      "\n",
      "\n",
      "In Depth\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "\n",
      "Programmes\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SERVICES\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Daily E-mail\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "News Ticker\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mobile/PDAs\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Only\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feedback\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "EDITIONS\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Change to UK\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Friday, 27 September, 2002, 11:51 GMT 12:51 UK\r\n",
      "\n",
      "Blondes 'to die out in 200 years'\n",
      "\n",
      "\n",
      "Scientists believe the last blondes will be in Finland\n",
      "\n",
      "\n",
      "\r\n",
      "\tThe last natural blondes will die out within 200 years, scientists believe. \r\n",
      "\r\n",
      "A study by experts in Germany suggests people with blonde hair are an endangered species and will become extinct by 2202.\r\n",
      "\r\n",
      "Researchers predict the last truly natural blonde will be born in Finland - the country with the highest proportion of blondes. \r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\tThe frequency of blondes may drop but they won't disappear\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t\n",
      "\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\tProf Jonathan Rees, University of Edinburgh\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t\r\n",
      "But they say too few people now carry the gene for blondes to last beyond the next two centuries. \r\n",
      "\r\n",
      "The problem is that blonde hair is caused by a recessive gene. \r\n",
      "\r\n",
      "In order for a child to have blonde hair, it must have the gene on both sides of the family in the grandparents' generation. \r\n",
      "Dyed rivals\n",
      "\r\n",
      "\r\n",
      "The researchers also believe that so-called bottle blondes may be to blame for the demise of their natural rivals. \r\n",
      "\r\n",
      "They suggest that dyed-blondes are more attractive to men who choose them as partners over true blondes. \r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bottle-blondes like Ann Widdecombe may be to blame\n",
      "\n",
      "\r\n",
      "\t\t\r\n",
      "\r\n",
      "\t\r\n",
      "But Jonathan Rees, professor of dermatology at the University of Edinburgh said it was unlikely blondes would die out completely. \r\n",
      "\r\n",
      "\"Genes don't die out unless there is a disadvantage of having that gene or by chance. They don't disappear,\" he told BBC News Online.\r\n",
      "\r\n",
      "\"The only reason blondes would disappear is if having the gene was a disadvantage and I do not think that is the case. \r\n",
      "\r\n",
      "\"The frequency of blondes may drop but they won't disappear.\"\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "See also:\n",
      "\n",
      "28 Mar 01 | Education\n",
      "What is it about blondes?\n",
      "\n",
      "\n",
      "09 Apr 99 | Health\n",
      "Platinum blondes are labelled as dumb\n",
      "\n",
      "\n",
      "17 Apr 02 | Health\n",
      "Hair dye cancer alert\n",
      "\n",
      "\n",
      "\n",
      "Internet links:\n",
      "University of Edinburgh\n",
      "The BBC is not responsible for the content of external internet sites\n",
      "\n",
      "\n",
      "Top Health stories now:\n",
      "Heart risk link to big families\n",
      "Back pain drug 'may aid diabetics'\n",
      "Congo Ebola outbreak confirmed\n",
      "Vegetables ward off Alzheimer's\n",
      "Polio campaign launched in Iraq\n",
      "Gene defect explains high blood pressure\n",
      "Botox 'may cause new wrinkles'\n",
      "Alien 'abductees' show real symptoms\n",
      "\n",
      "\n",
      "Links to more Health stories are at the foot of the page.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " E-mail this story to a friend\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Links to more Health stories\n",
      "\n",
      "\n",
      "\n",
      "In This Section\n",
      "Heart risk link to big families\n",
      "Back pain drug 'may aid diabetics'\n",
      "Congo Ebola outbreak confirmed\n",
      "Vegetables ward off Alzheimer's\n",
      "Polio campaign launched in Iraq\n",
      "Gene defect explains high blood pressure\n",
      "Botox 'may cause new wrinkles'\n",
      "Alien 'abductees' show real symptoms\n",
      "How sperm wriggle\n",
      "Bollywood told to stub it out\n",
      "Fears over tuna health risk to babies\n",
      "Public can be taught to spot strokes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "^^  \n",
      "Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "News Front Page\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "Africa\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "Americas\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "Asia-Pacific\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "Europe\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "Middle East\r\n",
      " | \r\n",
      "\n",
      "South Asia\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "UK\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "Business\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "Entertainment\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "Science/Nature\r\n",
      " | \r\n",
      "\n",
      "Technology\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "Health\r\n",
      " | \r\n",
      "\r\n",
      "\r\n",
      "Talking Point\r\n",
      " | \r\n",
      "\r\n",
      "Country Profiles\r\n",
      "\r\n",
      " | \r\n",
      "\r\n",
      "In Depth\r\n",
      "\r\n",
      " | \r\n",
      "\n",
      "Programmes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------\r\n",
      "\n",
      "\n",
      "To BBC Sport>>\r\n",
      " |\r\n",
      " To BBC Weather>>\r\n",
      " |\r\n",
      " To BBC World Service>>\r\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------\r\n",
      "\n",
      "\n",
      "© MMIII\r\n",
      " |\r\n",
      " News Sources\r\n",
      " |\r\n",
      " Privacy\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "# Raw html code\n",
    "html = urllib.request.urlopen(url).read()\n",
    "\n",
    "# Html removed tags\n",
    "text = BeautifulSoup(html, 'html.parser').get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence tokenizing ( Splitting sentences )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, I am Amir Esmaeili.', 'I work as a software engineer, who is trying to become an AI eng.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "    \n",
    "text = \"Hello, I am Amir Esmaeili. I work as a software engineer, who is trying to become an AI eng.\"\n",
    "\n",
    "# Default sentence splitter for standard texts\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# The bellow text is in the conversation format\n",
    "text = webtext.raw('overheard.txt')\n",
    "# Trains data set => This is used when text is not in standard format\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default sentence tokenizer \n",
    "sentence2 = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Girl: But you already have a Big Mac...\n",
      "Hobo: Oh, this is all theatrical.\n",
      "-----------\n",
      "Girl: But you already have a Big Mac...\n"
     ]
    }
   ],
   "source": [
    "print(sentence2[678] == sentences[678])\n",
    "print(sentence2[678])\n",
    "print(\"-----------\")\n",
    "print(sentences[678])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, different ways of tokenizing words are demonstrated.\n",
    "* standard\n",
    "* custom regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', \"'m\", ',', 'Amir', 'Esmaeili', '.', 'Computer', 'science', 'student', '@', 'IUST', 'and', 'full-stack', 'developer', 'at', 'Satplat', 'co', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hello, I'm, Amir Esmaeili. Computer science student @ IUST and full-stack developer at Satplat co.\"\n",
    "\n",
    "# Standart text tokenizing\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', \"'\", 'm', ',', 'Amir', 'Esmaeili', '.', 'Computer', 'science', 'student', '@', 'IUST', 'and', 'full', '-', 'stack', 'developer', 'at', 'Satplat', 'co', '.']\n"
     ]
    }
   ],
   "source": [
    "# Different kind of tokenizing.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "custom_tokenizer = WordPunctTokenizer()\n",
    "words = custom_tokenizer.tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"I'm\", 'Amir', 'Esmaeili', 'Computer', 'science', 'student', 'IUST', 'and', 'full-stack', 'developer', 'at', 'Satplat', 'co']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# Tokenizing by custom regex\n",
    "words = regexp_tokenize(text, pattern=\"[\\w'w-]+\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different type of stemmers available in nltk package**\n",
    "* The Porter Stemmer: Good enough for English\n",
    "* LancasterStemmer: More aggressive than the Porter\n",
    "* Snowball Stemmers: For 13 languages like Dutch, English and ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impress\n",
      "eat\n",
      "hol\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "pst = PorterStemmer()\n",
    "lst = LancasterStemmer()\n",
    "spanish_sst = SnowballStemmer('spanish')\n",
    "print(pst.stem('Impressive'))\n",
    "print(lst.stem('eating'))\n",
    "print(spanish_sst.stem('hola'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon']\n"
     ]
    }
   ],
   "source": [
    "words = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "        'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "        'meeting', 'stating', 'siezing', 'itemization',\n",
    "        'sensational', 'traditional', 'reference', 'colonizer']\n",
    "print(list(map(pst.stem, words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you seen in the previous block, some words are not even a meaningful word, that's because stemmer is only for \n",
    "stemming _ing_ , _s_ and _ed_.\n",
    "But Lemmatization uses a database called **wordnet** which connects every word to its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nouns ['foot', 'teeth', 'mouse']\n",
      "verbs ['be', 'inspire', 'fly', 'be']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nouns = ['feet', 'teeth', 'mice']\n",
    "from_verbs = ['are', 'inspiring', 'flying', 'was'] \n",
    "\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "print('nouns', [word_lemmatizer.lemmatize(x, pos='n') for x in nouns])\n",
    "print('verbs', [word_lemmatizer.lemmatize(x, pos='v') for x in from_verbs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, role of word is specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming vs. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Stemming is a process in which some suffix such as *es*, *s*, *ing* and ... are removed.\n",
    "But stemming is not going to produce meaningful words alway, and it just chops off the suffix without caring about\n",
    "the meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But Lemmatization uses a dictionary called **wordnet** in order to find the root of a word\n",
    "But it also requires us to know the role of the word, if it is *noun* or *verb* in before changing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some words in every language that doesn't help the algorithm, so we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/amiresm/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Of Speach Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('was', 'VBD'), ('eating', 'VBG'), ('dinner', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amiresm/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Data for pos tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"I was eating dinner\"\n",
    "words = word_tokenize(text)\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/amiresm/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/amiresm/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import pos_tag, UnigramTagger, BigramTagger, TrigramTagger, DefaultTagger\n",
    "\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sample tagged sentences\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "# Setting 90% of data for training and the 10% for testing\n",
    "train_data = brown_tagged_sents[:int(len(brown_tagged_sents)*.9)]\n",
    "test_data = brown_tagged_sents[int(len(brown_tagged_sents)*.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8361407355726104\n",
      "0.8452108043456593\n",
      "0.843317053722715\n"
     ]
    }
   ],
   "source": [
    "# Setting a default tagger -> This considers every word as noun\n",
    "def_tagger = DefaultTagger('NN')\n",
    "\n",
    "# Backoff model is a model which the current model gets help incase of failure\n",
    "uni_tagger = UnigramTagger(train=train_data, backoff=def_tagger)\n",
    "print(uni_tagger.evaluate(test_data))\n",
    "\n",
    "bi_tagger = BigramTagger(train=train_data, backoff=uni_tagger)\n",
    "print(bi_tagger.evaluate(test_data))\n",
    "\n",
    "tri_tagger = TrigramTagger(train=train_data, backoff=bi_tagger)\n",
    "print(tri_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnigramTagger**, **BigramTagger**, **TrigramTagger** are kinds of machine learning techniques used\n",
    "to train data. They are named based on the **number** of words they process before the main word for tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"UnigramTagger, BigramTagger, TrigramTagger are kinds of machine learning techniques used to train data. They are named based on the number of words they process before the main word for tagging.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UnigramTagger', ',', 'BigramTagger', ',', 'TrigramTagger', 'are', 'kinds', 'of', 'machine', 'learning', 'techniques', 'used', 'to', 'train', 'data', '.', 'They', 'are', 'named', 'based', 'on', 'the', 'number', 'of', 'words', 'they', 'process', 'before', 'the', 'main', 'word', 'for', 'tagging', '.']\n",
      "\n",
      "\n",
      "[('UnigramTagger', 'NN'), (',', ','), ('BigramTagger', 'NN'), (',', ','), ('TrigramTagger', 'NN'), ('are', 'BER'), ('kinds', 'NNS'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('techniques', 'NNS'), ('used', 'VBN'), ('to', 'TO'), ('train', 'NN'), ('data', 'NN'), ('.', '.'), ('They', 'PPSS'), ('are', 'BER'), ('named', 'VBN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'AT'), ('number', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('they', 'PPSS'), ('process', 'NN'), ('before', 'IN'), ('the', 'AT'), ('main', 'JJS'), ('word', 'NN'), ('for', 'IN'), ('tagging', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "words = []\n",
    "for sent in sentences:\n",
    "    words.extend(nltk.word_tokenize(sent))\n",
    "\n",
    "print(words)\n",
    "print('\\n')\n",
    "print(bi_tagger.tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity tagging is used to recognize entities in the sentence.\n",
    "#### Entities\n",
    "* geo: Geographical\n",
    "* org: Organization\n",
    "* per: Person\n",
    "* gpe: Geopolitical\n",
    "* tim: Time Indicator\n",
    "* art: Artifact\n",
    "* eve: Event\n",
    "* nat: Natural Phenomenal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/amiresm/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/amiresm/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ne_chunk\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Amir Esmaeili is a junior software developer at Satplat, who has been studying at IUST.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Amir/NNP)\n",
      "  (ORGANIZATION Esmaeili/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  junior/JJ\n",
      "  software/NN\n",
      "  developer/NN\n",
      "  at/IN\n",
      "  (ORGANIZATION Satplat/NNP)\n",
      "  ,/,\n",
      "  who/WP\n",
      "  has/VBZ\n",
      "  been/VBN\n",
      "  studying/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION IUST/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(ne_chunk(nltk.pos_tag(words), binary=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing Overview\n",
    "\n",
    "## Sections\n",
    "\n",
    "* Tokenize Sentences\n",
    "* Tokenize words\n",
    "* Lemmatize the words ( without pos )\n",
    "* Perfom POS tagging\n",
    "* Lemmatize words ( with pos )\n",
    "* Perform NER\n",
    "\n",
    "## Data\n",
    "\"Digikala Group is a leading e-commerce organization with a firm grip in multiple online industries including consumer goods, fashion & apparel, e-books, content publishing, digital advertising, big data, fintech, FMCG and logistics. The company operates through its subsidiaries including Digikala, DIGISTYLE, Fidibo and Digistyle representing nearly 92% of Iran’s online retail market share.\n",
    "With exponential growth of its customer base around its subsidiaries, the future for Digikala Group is bright and abundant. The company is dedicated to build upon its strong foothold in e-commerce and help more consumers and businesses around the MENA region to create more possibilities online, helping to boost the region’s promising economy with world-class standards in technology and service.\n",
    "At Digikala Group, the focus is on building for the future and increasing our presence in the markets we currently hold stake. Creative ideas, wild imaginations, teamwork, and thoughtful execution that are consumer-centric driven is encouraged at Digikala Group, maintaining a concentrated effort on innovative technology and service to the marketplace year-after-year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree, and gained practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry. In 1884 he emigrated to the United States, where he became a naturalized citizen. He worked for a short time at the Edison Machine Works in New York City before he struck out on his own. With the help of partners to finance and market his ideas, Tesla set up laboratories and companies in New York to develop a range of electrical and mechanical devices. His alternating current (AC) induction motor and related polyphase AC patents, licensed by Westinghouse Electric in 1888, earned him a considerable amount of money and became the cornerstone of the polyphase system which that company eventually marketed.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amiresm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/amiresm/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amiresm/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/amiresm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/amiresm/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/amiresm/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk\n",
    "\n",
    "\n",
    "# Preparing the needed datasets\n",
    "datasets = ['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords', 'maxent_ne_chunker', 'words']\n",
    "for pkg in datasets:\n",
    "    nltk.download(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences= ['Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree, and gained practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry.', 'In 1884 he emigrated to the United States, where he became a naturalized citizen.', 'He worked for a short time at the Edison Machine Works in New York City before he struck out on his own.', 'With the help of partners to finance and market his ideas, Tesla set up laboratories and companies in New York to develop a range of electrical and mechanical devices.', 'His alternating current (AC) induction motor and related polyphase AC patents, licensed by Westinghouse Electric in 1888, earned him a considerable amount of money and became the cornerstone of the polyphase system which that company eventually marketed.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"sentences=\",sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words= ['Born', 'and', 'raised', 'in', 'the', 'Austrian', 'Empire', ',', 'Tesla', 'studied', 'engineering', 'and', 'physics', 'in', 'the', '1870s', 'without', 'receiving', 'a', 'degree', ',', 'and', 'gained', 'practical', 'experience', 'in', 'the', 'early', '1880s', 'working', 'in', 'telephony', 'and', 'at', 'Continental', 'Edison', 'in', 'the', 'new', 'electric', 'power', 'industry', '.', 'In', '1884', 'he', 'emigrated', 'to', 'the', 'United', 'States', ',', 'where', 'he', 'became', 'a', 'naturalized', 'citizen', '.', 'He', 'worked', 'for', 'a', 'short', 'time', 'at', 'the', 'Edison', 'Machine', 'Works', 'in', 'New', 'York', 'City', 'before', 'he', 'struck', 'out', 'on', 'his', 'own', '.', 'With', 'the', 'help', 'of', 'partners', 'to', 'finance', 'and', 'market', 'his', 'ideas', ',', 'Tesla', 'set', 'up', 'laboratories', 'and', 'companies', 'in', 'New', 'York', 'to', 'develop', 'a', 'range', 'of', 'electrical', 'and', 'mechanical', 'devices', '.', 'His', 'alternating', 'current', '(', 'AC', ')', 'induction', 'motor', 'and', 'related', 'polyphase', 'AC', 'patents', ',', 'licensed', 'by', 'Westinghouse', 'Electric', 'in', '1888', ',', 'earned', 'him', 'a', 'considerable', 'amount', 'of', 'money', 'and', 'became', 'the', 'cornerstone', 'of', 'the', 'polyphase', 'system', 'which', 'that', 'company', 'eventually', 'marketed', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Word\n",
    "words = list() \n",
    "for sentence in sentences:\n",
    "    words.extend(word_tokenize(sentence))\n",
    "print(\"words=\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma(without pos)= ['Born', 'and', 'raised', 'in', 'the', 'Austrian', 'Empire', ',', 'Tesla', 'studied', 'engineering', 'and', 'physic', 'in', 'the', '1870s', 'without', 'receiving', 'a', 'degree', ',', 'and', 'gained', 'practical', 'experience', 'in', 'the', 'early', '1880s', 'working', 'in', 'telephony', 'and', 'at', 'Continental', 'Edison', 'in', 'the', 'new', 'electric', 'power', 'industry', '.', 'In', '1884', 'he', 'emigrated', 'to', 'the', 'United', 'States', ',', 'where', 'he', 'became', 'a', 'naturalized', 'citizen', '.', 'He', 'worked', 'for', 'a', 'short', 'time', 'at', 'the', 'Edison', 'Machine', 'Works', 'in', 'New', 'York', 'City', 'before', 'he', 'struck', 'out', 'on', 'his', 'own', '.', 'With', 'the', 'help', 'of', 'partner', 'to', 'finance', 'and', 'market', 'his', 'idea', ',', 'Tesla', 'set', 'up', 'laboratory', 'and', 'company', 'in', 'New', 'York', 'to', 'develop', 'a', 'range', 'of', 'electrical', 'and', 'mechanical', 'device', '.', 'His', 'alternating', 'current', '(', 'AC', ')', 'induction', 'motor', 'and', 'related', 'polyphase', 'AC', 'patent', ',', 'licensed', 'by', 'Westinghouse', 'Electric', 'in', '1888', ',', 'earned', 'him', 'a', 'considerable', 'amount', 'of', 'money', 'and', 'became', 'the', 'cornerstone', 'of', 'the', 'polyphase', 'system', 'which', 'that', 'company', 'eventually', 'marketed', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization without pos tagging\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemma = list()\n",
    "for word in words:\n",
    "    l = wordnet_lemmatizer.lemmatize(word)\n",
    "    lemma.append(l)\n",
    "print(\"lemma(without pos)=\", lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged_words= [('Born', 'NNP'), ('and', 'CC'), ('raised', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('Austrian', 'JJ'), ('Empire', 'NNP'), (',', ','), ('Tesla', 'NNP'), ('studied', 'VBD'), ('engineering', 'NN'), ('and', 'CC'), ('physics', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('1870s', 'CD'), ('without', 'IN'), ('receiving', 'VBG'), ('a', 'DT'), ('degree', 'NN'), (',', ','), ('and', 'CC'), ('gained', 'VBD'), ('practical', 'JJ'), ('experience', 'NN'), ('in', 'IN'), ('the', 'DT'), ('early', 'JJ'), ('1880s', 'CD'), ('working', 'VBG'), ('in', 'IN'), ('telephony', 'NN'), ('and', 'CC'), ('at', 'IN'), ('Continental', 'NNP'), ('Edison', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('electric', 'JJ'), ('power', 'NN'), ('industry', 'NN'), ('.', '.'), ('In', 'IN'), ('1884', 'CD'), ('he', 'PRP'), ('emigrated', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (',', ','), ('where', 'WRB'), ('he', 'PRP'), ('became', 'VBD'), ('a', 'DT'), ('naturalized', 'JJ'), ('citizen', 'NN'), ('.', '.'), ('He', 'PRP'), ('worked', 'VBD'), ('for', 'IN'), ('a', 'DT'), ('short', 'JJ'), ('time', 'NN'), ('at', 'IN'), ('the', 'DT'), ('Edison', 'NNP'), ('Machine', 'NNP'), ('Works', 'NNP'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('City', 'NNP'), ('before', 'IN'), ('he', 'PRP'), ('struck', 'VBD'), ('out', 'RP'), ('on', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), ('.', '.'), ('With', 'IN'), ('the', 'DT'), ('help', 'NN'), ('of', 'IN'), ('partners', 'NNS'), ('to', 'TO'), ('finance', 'VB'), ('and', 'CC'), ('market', 'NN'), ('his', 'PRP$'), ('ideas', 'NNS'), (',', ','), ('Tesla', 'NNP'), ('set', 'VBD'), ('up', 'RP'), ('laboratories', 'NNS'), ('and', 'CC'), ('companies', 'NNS'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('to', 'TO'), ('develop', 'VB'), ('a', 'DT'), ('range', 'NN'), ('of', 'IN'), ('electrical', 'JJ'), ('and', 'CC'), ('mechanical', 'JJ'), ('devices', 'NNS'), ('.', '.'), ('His', 'PRP$'), ('alternating', 'VBG'), ('current', 'JJ'), ('(', '('), ('AC', 'NNP'), (')', ')'), ('induction', 'NN'), ('motor', 'NN'), ('and', 'CC'), ('related', 'JJ'), ('polyphase', 'NN'), ('AC', 'NNP'), ('patents', 'NNS'), (',', ','), ('licensed', 'VBN'), ('by', 'IN'), ('Westinghouse', 'NNP'), ('Electric', 'NNP'), ('in', 'IN'), ('1888', 'CD'), (',', ','), ('earned', 'VBD'), ('him', 'PRP'), ('a', 'DT'), ('considerable', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('money', 'NN'), ('and', 'CC'), ('became', 'VBD'), ('the', 'DT'), ('cornerstone', 'NN'), ('of', 'IN'), ('the', 'DT'), ('polyphase', 'NN'), ('system', 'NN'), ('which', 'WDT'), ('that', 'DT'), ('company', 'NN'), ('eventually', 'RB'), ('marketed', 'VBD'), ('.', '.')]\n",
      "n= 155\n"
     ]
    }
   ],
   "source": [
    "# Performing POS tagging\n",
    "tagged_words = pos_tag(words)\n",
    "print(\"tagged_words=\", tagged_words)\n",
    "print(\"n=\", len(tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Born', 'NNP'), ('raised', 'VBN'), ('Austrian', 'JJ'), ('Empire', 'NNP'), ('Tesla', 'NNP'), ('studied', 'VBD'), ('engineering', 'NN'), ('physics', 'NNS'), ('1870s', 'CD'), ('without', 'IN'), ('receiving', 'VBG'), ('degree', 'NN'), ('gained', 'VBD'), ('practical', 'JJ'), ('experience', 'NN'), ('the', 'DT'), ('early', 'JJ'), ('1880s', 'CD'), ('working', 'VBG'), ('telephony', 'NN'), ('Continental', 'NNP'), ('Edison', 'NNP'), ('the', 'DT'), ('new', 'JJ'), ('electric', 'JJ'), ('power', 'NN'), ('industry', 'NN'), ('In', 'IN'), ('1884', 'CD'), ('emigrated', 'VBD'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('where', 'WRB'), ('became', 'VBD'), ('naturalized', 'JJ'), ('citizen', 'NN'), ('He', 'PRP'), ('worked', 'VBD'), ('short', 'JJ'), ('time', 'NN'), ('at', 'IN'), ('the', 'DT'), ('Edison', 'NNP'), ('Machine', 'NNP'), ('Works', 'NNP'), ('New', 'NNP'), ('York', 'NNP'), ('City', 'NNP'), ('he', 'PRP'), ('struck', 'VBD'), ('on', 'IN'), ('own', 'JJ'), ('With', 'IN'), ('the', 'DT'), ('help', 'NN'), ('partners', 'NNS'), ('finance', 'VB'), ('market', 'NN'), ('ideas', 'NNS'), ('Tesla', 'NNP'), ('set', 'VBD'), ('laboratories', 'NNS'), ('companies', 'NNS'), ('New', 'NNP'), ('York', 'NNP'), ('develop', 'VB'), ('a', 'DT'), ('range', 'NN'), ('electrical', 'JJ'), ('mechanical', 'JJ'), ('devices', 'NNS'), ('His', 'PRP$'), ('alternating', 'VBG'), ('current', 'JJ'), ('AC', 'NNP'), ('induction', 'NN'), ('motor', 'NN'), ('related', 'JJ'), ('polyphase', 'NN'), ('AC', 'NNP'), ('patents', 'NNS'), ('licensed', 'VBN'), ('Westinghouse', 'NNP'), ('Electric', 'NNP'), ('1888', 'CD'), ('earned', 'VBD'), ('a', 'DT'), ('considerable', 'JJ'), ('amount', 'NN'), ('money', 'NN'), ('and', 'CC'), ('became', 'VBD'), ('the', 'DT'), ('cornerstone', 'NN'), ('the', 'DT'), ('polyphase', 'NN'), ('system', 'NN'), ('that', 'DT'), ('company', 'NN'), ('eventually', 'RB'), ('marketed', 'VBD')]\n",
      "n= 102\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "# Removing stopwords and punktuations\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punk = string.punctuation\n",
    "for word, tag in tagged_words:\n",
    "    if word in stop_words or word in punk:\n",
    "        tagged_words.remove((word, tag))\n",
    "print(tagged_words)\n",
    "print(\"n=\", len(tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_pos(treebank_tag: str) -> 'string':\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_pos= ['Born', 'raise', 'Austrian', 'Empire', 'Tesla', 'study', 'engineering', 'physic', 'receive', 'degree', 'gain', 'practical', 'experience', 'early', 'work', 'telephony', 'Continental', 'Edison', 'new', 'electric', 'power', 'industry', 'emigrate', 'United', 'States', 'become', 'naturalized', 'citizen', 'work', 'short', 'time', 'Edison', 'Machine', 'Works', 'New', 'York', 'City', 'strike', 'own', 'help', 'partner', 'finance', 'market', 'idea', 'Tesla', 'set', 'laboratory', 'company', 'New', 'York', 'develop', 'range', 'electrical', 'mechanical', 'device', 'alternate', 'current', 'AC', 'induction', 'motor', 'related', 'polyphase', 'AC', 'patent', 'license', 'Westinghouse', 'Electric', 'earn', 'considerable', 'amount', 'money', 'become', 'cornerstone', 'polyphase', 'system', 'company', 'eventually', 'market']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing with pos tagging\n",
    "\n",
    "lemma_pos = list()\n",
    "\n",
    "for word, tag in tagged_words:\n",
    "    pos = get_pos(tag)\n",
    "    if pos != '':\n",
    "        l = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "        lemma_pos.append(l)\n",
    "print(\"lemma_pos=\", lemma_pos)\n",
    "\n",
    "# It is not completely Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Born/NNP)\n",
      "  raised/VBN\n",
      "  (GPE Austrian/JJ)\n",
      "  (ORGANIZATION Empire/NNP Tesla/NNP)\n",
      "  studied/VBD\n",
      "  engineering/NN\n",
      "  physics/NNS\n",
      "  1870s/CD\n",
      "  without/IN\n",
      "  receiving/VBG\n",
      "  degree/NN\n",
      "  gained/VBD\n",
      "  practical/JJ\n",
      "  experience/NN\n",
      "  the/DT\n",
      "  early/JJ\n",
      "  1880s/CD\n",
      "  working/VBG\n",
      "  telephony/NN\n",
      "  (ORGANIZATION Continental/NNP)\n",
      "  Edison/NNP\n",
      "  the/DT\n",
      "  new/JJ\n",
      "  electric/JJ\n",
      "  power/NN\n",
      "  industry/NN\n",
      "  In/IN\n",
      "  1884/CD\n",
      "  emigrated/VBD\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  where/WRB\n",
      "  became/VBD\n",
      "  naturalized/JJ\n",
      "  citizen/NN\n",
      "  He/PRP\n",
      "  worked/VBD\n",
      "  short/JJ\n",
      "  time/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Edison/NNP Machine/NNP Works/NNP New/NNP York/NNP)\n",
      "  City/NNP\n",
      "  he/PRP\n",
      "  struck/VBD\n",
      "  on/IN\n",
      "  own/JJ\n",
      "  With/IN\n",
      "  the/DT\n",
      "  help/NN\n",
      "  partners/NNS\n",
      "  finance/VB\n",
      "  market/NN\n",
      "  ideas/NNS\n",
      "  (PERSON Tesla/NNP)\n",
      "  set/VBD\n",
      "  laboratories/NNS\n",
      "  companies/NNS\n",
      "  (GPE New/NNP York/NNP)\n",
      "  develop/VB\n",
      "  a/DT\n",
      "  range/NN\n",
      "  electrical/JJ\n",
      "  mechanical/JJ\n",
      "  devices/NNS\n",
      "  His/PRP$\n",
      "  alternating/VBG\n",
      "  current/JJ\n",
      "  AC/NNP\n",
      "  induction/NN\n",
      "  motor/NN\n",
      "  related/JJ\n",
      "  polyphase/NN\n",
      "  AC/NNP\n",
      "  patents/NNS\n",
      "  licensed/VBN\n",
      "  (ORGANIZATION Westinghouse/NNP)\n",
      "  Electric/NNP\n",
      "  1888/CD\n",
      "  earned/VBD\n",
      "  a/DT\n",
      "  considerable/JJ\n",
      "  amount/NN\n",
      "  money/NN\n",
      "  and/CC\n",
      "  became/VBD\n",
      "  the/DT\n",
      "  cornerstone/NN\n",
      "  the/DT\n",
      "  polyphase/NN\n",
      "  system/NN\n",
      "  that/DT\n",
      "  company/NN\n",
      "  eventually/RB\n",
      "  marketed/VBD)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "print(ne_chunk(tagged_words, binary=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
